{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SarcasmModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adikiit123/SharpML/blob/master/SarcasmModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "z6J0VsrnF91Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        },
        "outputId": "3762659d-e0cc-41a3-9313-e64749f27aae"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re \n",
        "import pandas as pd\n",
        "from keras.layers import Embedding,Dense,Flatten, LSTM, Dropout\n",
        "from keras.losses import binary_crossentropy,mse\n",
        "from keras.activations import sigmoid,relu\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import text\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "#dataPath = uploaded['Sarcasm_Headlines_Dataset.json'].decode('utf-8')\n",
        "\n",
        "dataPath = os.getcwd() + \"/Sarcasm_Headlines_Dataset.json\"\n",
        "data = pd.read_json(dataPath, lines=True)\n",
        "\n",
        "headLine = data['headline']\n",
        "sarcastic = data['is_sarcastic']\n",
        "\n",
        "headLine = [re.sub('[^a-zA-z0-9\\s]', '', x) for x in headLine]\n",
        "headLine = [x.lower() for x in headLine]\n",
        "\n",
        "maxVectorSize = 10\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(headLine)\n",
        "tokenizedSequences = tokenizer.texts_to_sequences(headLine)\n",
        "paddedheadLine = sequence.pad_sequences(tokenizedSequences,maxVectorSize,padding = 'post')\n",
        "\n",
        "#print(paddedheadLine[:10])\n",
        "\n",
        "vocabSize = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "\n",
        "sarcasmModel = Sequential()\n",
        "embeddingLayer = Embedding(vocabSize,32,input_length = maxVectorSize)\n",
        "lstmLayer = LSTM(32)\n",
        "droupOut = Dropout(0.4)\n",
        "outputLayer = Dense(1,activation = sigmoid)\n",
        "\n",
        "sarcasmModel.add(embeddingLayer)\n",
        "sarcasmModel.add(lstmLayer)\n",
        "sarcasmModel.add(droupOut)\n",
        "sarcasmModel.add(outputLayer)\n",
        "sgdOptimizer = SGD(0.1,1.0,0.01,True)\n",
        "\n",
        "print(sarcasmModel.summary())\n",
        "\n",
        "sarcasmModel.compile(optimizer = sgdOptimizer,loss = mse,metrics = ['accuracy'])\n",
        "sarcasmModel.fit(paddedheadLine,sarcastic,100,25)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 10, 32)            908768    \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 917,121\n",
            "Trainable params: 917,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "26709/26709 [==============================] - 8s 303us/step - loss: 0.2452 - acc: 0.5592\n",
            "Epoch 2/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.1844 - acc: 0.7322\n",
            "Epoch 3/25\n",
            "26709/26709 [==============================] - 5s 186us/step - loss: 0.1289 - acc: 0.8254\n",
            "Epoch 4/25\n",
            "26709/26709 [==============================] - 5s 185us/step - loss: 0.1035 - acc: 0.8651\n",
            "Epoch 5/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.0871 - acc: 0.8895\n",
            "Epoch 6/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.0776 - acc: 0.9056\n",
            "Epoch 7/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.0727 - acc: 0.9131\n",
            "Epoch 8/25\n",
            "26709/26709 [==============================] - 5s 186us/step - loss: 0.0687 - acc: 0.9195\n",
            "Epoch 9/25\n",
            "26709/26709 [==============================] - 5s 185us/step - loss: 0.0640 - acc: 0.9266\n",
            "Epoch 10/25\n",
            "26709/26709 [==============================] - 5s 198us/step - loss: 0.0625 - acc: 0.9266\n",
            "Epoch 11/25\n",
            "26709/26709 [==============================] - 5s 196us/step - loss: 0.0611 - acc: 0.9317\n",
            "Epoch 12/25\n",
            "26709/26709 [==============================] - 5s 185us/step - loss: 0.0580 - acc: 0.9357\n",
            "Epoch 13/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.0589 - acc: 0.9349\n",
            "Epoch 14/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0599 - acc: 0.9339\n",
            "Epoch 15/25\n",
            "26709/26709 [==============================] - 5s 182us/step - loss: 0.0590 - acc: 0.9340\n",
            "Epoch 16/25\n",
            "26709/26709 [==============================] - 5s 193us/step - loss: 0.0591 - acc: 0.9331\n",
            "Epoch 17/25\n",
            "26709/26709 [==============================] - 5s 188us/step - loss: 0.0590 - acc: 0.9338\n",
            "Epoch 18/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0614 - acc: 0.9313\n",
            "Epoch 19/25\n",
            "26709/26709 [==============================] - 5s 184us/step - loss: 0.0603 - acc: 0.9319\n",
            "Epoch 20/25\n",
            "26709/26709 [==============================] - 5s 185us/step - loss: 0.0608 - acc: 0.9310\n",
            "Epoch 21/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0599 - acc: 0.9325\n",
            "Epoch 22/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0584 - acc: 0.9348\n",
            "Epoch 23/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0612 - acc: 0.9324\n",
            "Epoch 24/25\n",
            "26709/26709 [==============================] - 5s 183us/step - loss: 0.0619 - acc: 0.9310\n",
            "Epoch 25/25\n",
            "26709/26709 [==============================] - 5s 185us/step - loss: 0.0652 - acc: 0.9272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f97e5824fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}